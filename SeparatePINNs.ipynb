{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d59b2ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]\n",
      " [6.25864161]]\n",
      "2\n",
      "[[0.         0.        ]\n",
      " [0.02454369 0.        ]\n",
      " [0.04908739 0.        ]\n",
      " [0.07363108 0.        ]\n",
      " [0.09817477 0.        ]\n",
      " [0.12271846 0.        ]\n",
      " [0.14726216 0.        ]\n",
      " [0.17180585 0.        ]\n",
      " [0.19634954 0.        ]\n",
      " [0.22089323 0.        ]\n",
      " [0.24543693 0.        ]\n",
      " [0.26998062 0.        ]\n",
      " [0.29452431 0.        ]\n",
      " [0.319068   0.        ]\n",
      " [0.3436117  0.        ]\n",
      " [0.36815539 0.        ]\n",
      " [0.39269908 0.        ]\n",
      " [0.41724277 0.        ]\n",
      " [0.44178647 0.        ]\n",
      " [0.46633016 0.        ]\n",
      " [0.49087385 0.        ]\n",
      " [0.51541754 0.        ]\n",
      " [0.53996124 0.        ]\n",
      " [0.56450493 0.        ]\n",
      " [0.58904862 0.        ]\n",
      " [0.61359232 0.        ]\n",
      " [0.63813601 0.        ]\n",
      " [0.6626797  0.        ]\n",
      " [0.68722339 0.        ]\n",
      " [0.71176709 0.        ]\n",
      " [0.73631078 0.        ]\n",
      " [0.76085447 0.        ]\n",
      " [0.78539816 0.        ]\n",
      " [0.80994186 0.        ]\n",
      " [0.83448555 0.        ]\n",
      " [0.85902924 0.        ]\n",
      " [0.88357293 0.        ]\n",
      " [0.90811663 0.        ]\n",
      " [0.93266032 0.        ]\n",
      " [0.95720401 0.        ]\n",
      " [0.9817477  0.        ]\n",
      " [1.0062914  0.        ]\n",
      " [1.03083509 0.        ]\n",
      " [1.05537878 0.        ]\n",
      " [1.07992247 0.        ]\n",
      " [1.10446617 0.        ]\n",
      " [1.12900986 0.        ]\n",
      " [1.15355355 0.        ]\n",
      " [1.17809725 0.        ]\n",
      " [1.20264094 0.        ]\n",
      " [1.22718463 0.        ]\n",
      " [1.25172832 0.        ]\n",
      " [1.27627202 0.        ]\n",
      " [1.30081571 0.        ]\n",
      " [1.3253594  0.        ]\n",
      " [1.34990309 0.        ]\n",
      " [1.37444679 0.        ]\n",
      " [1.39899048 0.        ]\n",
      " [1.42353417 0.        ]\n",
      " [1.44807786 0.        ]\n",
      " [1.47262156 0.        ]\n",
      " [1.49716525 0.        ]\n",
      " [1.52170894 0.        ]\n",
      " [1.54625263 0.        ]\n",
      " [1.57079633 0.        ]\n",
      " [1.59534002 0.        ]\n",
      " [1.61988371 0.        ]\n",
      " [1.6444274  0.        ]\n",
      " [1.6689711  0.        ]\n",
      " [1.69351479 0.        ]\n",
      " [1.71805848 0.        ]\n",
      " [1.74260218 0.        ]\n",
      " [1.76714587 0.        ]\n",
      " [1.79168956 0.        ]\n",
      " [1.81623325 0.        ]\n",
      " [1.84077695 0.        ]\n",
      " [1.86532064 0.        ]\n",
      " [1.88986433 0.        ]\n",
      " [1.91440802 0.        ]\n",
      " [1.93895172 0.        ]\n",
      " [1.96349541 0.        ]\n",
      " [1.9880391  0.        ]\n",
      " [2.01258279 0.        ]\n",
      " [2.03712649 0.        ]\n",
      " [2.06167018 0.        ]\n",
      " [2.08621387 0.        ]\n",
      " [2.11075756 0.        ]\n",
      " [2.13530126 0.        ]\n",
      " [2.15984495 0.        ]\n",
      " [2.18438864 0.        ]\n",
      " [2.20893233 0.        ]\n",
      " [2.23347603 0.        ]\n",
      " [2.25801972 0.        ]\n",
      " [2.28256341 0.        ]\n",
      " [2.3071071  0.        ]\n",
      " [2.3316508  0.        ]\n",
      " [2.35619449 0.        ]\n",
      " [2.38073818 0.        ]\n",
      " [2.40528188 0.        ]\n",
      " [2.42982557 0.        ]\n",
      " [2.45436926 0.        ]\n",
      " [2.47891295 0.        ]\n",
      " [2.50345665 0.        ]\n",
      " [2.52800034 0.        ]\n",
      " [2.55254403 0.        ]\n",
      " [2.57708772 0.        ]\n",
      " [2.60163142 0.        ]\n",
      " [2.62617511 0.        ]\n",
      " [2.6507188  0.        ]\n",
      " [2.67526249 0.        ]\n",
      " [2.69980619 0.        ]\n",
      " [2.72434988 0.        ]\n",
      " [2.74889357 0.        ]\n",
      " [2.77343726 0.        ]\n",
      " [2.79798096 0.        ]\n",
      " [2.82252465 0.        ]\n",
      " [2.84706834 0.        ]\n",
      " [2.87161203 0.        ]\n",
      " [2.89615573 0.        ]\n",
      " [2.92069942 0.        ]\n",
      " [2.94524311 0.        ]\n",
      " [2.96978681 0.        ]\n",
      " [2.9943305  0.        ]\n",
      " [3.01887419 0.        ]\n",
      " [3.04341788 0.        ]\n",
      " [3.06796158 0.        ]\n",
      " [3.09250527 0.        ]\n",
      " [3.11704896 0.        ]\n",
      " [3.14159265 0.        ]\n",
      " [3.16613635 0.        ]\n",
      " [3.19068004 0.        ]\n",
      " [3.21522373 0.        ]\n",
      " [3.23976742 0.        ]\n",
      " [3.26431112 0.        ]\n",
      " [3.28885481 0.        ]\n",
      " [3.3133985  0.        ]\n",
      " [3.33794219 0.        ]\n",
      " [3.36248589 0.        ]\n",
      " [3.38702958 0.        ]\n",
      " [3.41157327 0.        ]\n",
      " [3.43611696 0.        ]\n",
      " [3.46066066 0.        ]\n",
      " [3.48520435 0.        ]\n",
      " [3.50974804 0.        ]\n",
      " [3.53429174 0.        ]\n",
      " [3.55883543 0.        ]\n",
      " [3.58337912 0.        ]\n",
      " [3.60792281 0.        ]\n",
      " [3.63246651 0.        ]\n",
      " [3.6570102  0.        ]\n",
      " [3.68155389 0.        ]\n",
      " [3.70609758 0.        ]\n",
      " [3.73064128 0.        ]\n",
      " [3.75518497 0.        ]\n",
      " [3.77972866 0.        ]\n",
      " [3.80427235 0.        ]\n",
      " [3.82881605 0.        ]\n",
      " [3.85335974 0.        ]\n",
      " [3.87790343 0.        ]\n",
      " [3.90244712 0.        ]\n",
      " [3.92699082 0.        ]\n",
      " [3.95153451 0.        ]\n",
      " [3.9760782  0.        ]\n",
      " [4.00062189 0.        ]\n",
      " [4.02516559 0.        ]\n",
      " [4.04970928 0.        ]\n",
      " [4.07425297 0.        ]\n",
      " [4.09879667 0.        ]\n",
      " [4.12334036 0.        ]\n",
      " [4.14788405 0.        ]\n",
      " [4.17242774 0.        ]\n",
      " [4.19697144 0.        ]\n",
      " [4.22151513 0.        ]\n",
      " [4.24605882 0.        ]\n",
      " [4.27060251 0.        ]\n",
      " [4.29514621 0.        ]\n",
      " [4.3196899  0.        ]\n",
      " [4.34423359 0.        ]\n",
      " [4.36877728 0.        ]\n",
      " [4.39332098 0.        ]\n",
      " [4.41786467 0.        ]\n",
      " [4.44240836 0.        ]\n",
      " [4.46695205 0.        ]\n",
      " [4.49149575 0.        ]\n",
      " [4.51603944 0.        ]\n",
      " [4.54058313 0.        ]\n",
      " [4.56512682 0.        ]\n",
      " [4.58967052 0.        ]\n",
      " [4.61421421 0.        ]\n",
      " [4.6387579  0.        ]\n",
      " [4.6633016  0.        ]\n",
      " [4.68784529 0.        ]\n",
      " [4.71238898 0.        ]\n",
      " [4.73693267 0.        ]\n",
      " [4.76147637 0.        ]\n",
      " [4.78602006 0.        ]\n",
      " [4.81056375 0.        ]\n",
      " [4.83510744 0.        ]\n",
      " [4.85965114 0.        ]\n",
      " [4.88419483 0.        ]\n",
      " [4.90873852 0.        ]\n",
      " [4.93328221 0.        ]\n",
      " [4.95782591 0.        ]\n",
      " [4.9823696  0.        ]\n",
      " [5.00691329 0.        ]\n",
      " [5.03145698 0.        ]\n",
      " [5.05600068 0.        ]\n",
      " [5.08054437 0.        ]\n",
      " [5.10508806 0.        ]\n",
      " [5.12963175 0.        ]\n",
      " [5.15417545 0.        ]\n",
      " [5.17871914 0.        ]\n",
      " [5.20326283 0.        ]\n",
      " [5.22780653 0.        ]\n",
      " [5.25235022 0.        ]\n",
      " [5.27689391 0.        ]\n",
      " [5.3014376  0.        ]\n",
      " [5.3259813  0.        ]\n",
      " [5.35052499 0.        ]\n",
      " [5.37506868 0.        ]\n",
      " [5.39961237 0.        ]\n",
      " [5.42415607 0.        ]\n",
      " [5.44869976 0.        ]\n",
      " [5.47324345 0.        ]\n",
      " [5.49778714 0.        ]\n",
      " [5.52233084 0.        ]\n",
      " [5.54687453 0.        ]\n",
      " [5.57141822 0.        ]\n",
      " [5.59596191 0.        ]\n",
      " [5.62050561 0.        ]\n",
      " [5.6450493  0.        ]\n",
      " [5.66959299 0.        ]\n",
      " [5.69413668 0.        ]\n",
      " [5.71868038 0.        ]\n",
      " [5.74322407 0.        ]\n",
      " [5.76776776 0.        ]\n",
      " [5.79231146 0.        ]\n",
      " [5.81685515 0.        ]\n",
      " [5.84139884 0.        ]\n",
      " [5.86594253 0.        ]\n",
      " [5.89048623 0.        ]\n",
      " [5.91502992 0.        ]\n",
      " [5.93957361 0.        ]\n",
      " [5.9641173  0.        ]\n",
      " [5.988661   0.        ]\n",
      " [6.01320469 0.        ]\n",
      " [6.03774838 0.        ]\n",
      " [6.06229207 0.        ]\n",
      " [6.08683577 0.        ]\n",
      " [6.11137946 0.        ]\n",
      " [6.13592315 0.        ]\n",
      " [6.16046684 0.        ]\n",
      " [6.18501054 0.        ]\n",
      " [6.20955423 0.        ]\n",
      " [6.23409792 0.        ]\n",
      " [6.25864161 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Physics informed neural networks\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# number of points\n",
    "nx=256\n",
    "nt=100\n",
    "\n",
    "# equation variable\n",
    "rho=1\n",
    "\n",
    "# Sample points for our three conditions \n",
    "Nf  = 100   # pde condition\n",
    "Nbc = 2     # boundary condition\n",
    "N0  = 2     # initial condition \n",
    "\n",
    "# Reaction function\n",
    "def reaction(u0, rho, dt):\n",
    "    \"\"\" solution of du/dt = rho*u*(1-u) with u0 = u(x,t=0) \"\"\"\n",
    "    numerator = u0 * np.exp(rho * dt)\n",
    "    denominator = u0 * np.exp(rho * dt) + 1 - u0\n",
    "    u = numerator / denominator\n",
    "    return u\n",
    "\n",
    "def reaction_solution(rho, nx, nt):\n",
    "    \"\"\" discrete solution of du/dt = rho*u*(1-u) with u0 = u(x,t=0) \"\"\"\n",
    "    L = 2*np.pi # Lenght \n",
    "    T = 1       # Time\n",
    "    dx = L/nx\n",
    "    dt = T/nt\n",
    "    x = np.arange(0, 2*np.pi, dx) # (256,)\n",
    "    t = np.linspace(0, T, nt).reshape(-1, 1) # (100,1)\n",
    "    X, T = np.meshgrid(x, t) # X (100,256) = [[0......2pi],\n",
    "                             #                [0......2pi],\n",
    "                             #                    ...\n",
    "                             #                [0......2pi]], \n",
    "                             #\n",
    "                             # T (100,256) = [[0........0],\n",
    "                             #                [0.01..0.01],\n",
    "                             #                [1........1]]\n",
    "    \n",
    "    # initial value: u0 = u(x,t=0)\n",
    "    u0 = lambda x: np.exp(-np.power((x - np.pi)/(np.pi/4), 2.)/2.) # + np.random.uniform(-0.05,0.05,256)\n",
    "    u0 = u0(x) # (256,) : u(0),...,u(2pi)\n",
    "    \n",
    "    # function\n",
    "    u = reaction(u0, rho, T) # (100,256)\n",
    "    u = u.flatten() # (25600,)\n",
    "    return u\n",
    "\n",
    "\n",
    "# input parameters\n",
    "x = np.linspace(0, 2*np.pi, nx, endpoint=False).reshape(-1, 1) # (256,1): [0,...,2pi].T\n",
    "x_noboundary = x[1:]                                           # (255,1): [0.0245,...,2pi].T \n",
    "t = np.linspace(0, 1, nt).reshape(-1, 1) # (100,1): [0,...,1].T\n",
    "t_noinitial = t[1:]                      # (99,1):  [0.01,...,1].T\n",
    "\n",
    "\n",
    "########## X* grid with initial condition\n",
    "\n",
    "X, T = np.meshgrid(x, t) # X (100,256) = [[0......2pi],\n",
    "                         #                [0......2pi],\n",
    "                         #                [   ...    ]\n",
    "                         #                [0......2pi]], \n",
    "                         #\n",
    "                         # T (100,256) = [[0........0],\n",
    "                         #                [0.01..0.01],\n",
    "                         #                [    ...   ]\n",
    "                         #                [1........1]]\n",
    "X_star = np.hstack((X.flatten()[:, None], T.flatten()[:, None])) \n",
    "# X_star (25600,2) = [[0,     0],\n",
    "#                     [0.0245,0],\n",
    "#                     [...,   0],\n",
    "#                     [2pi,   0],\n",
    "#                       ....\n",
    "#                       ....\n",
    "#                     [0,     1]\n",
    "#                     [0.0245,1],\n",
    "#                     [...,   1],\n",
    "#                     [2pi,   1]]  \n",
    "\n",
    "\n",
    "########### X* grid without initial condition\n",
    "\n",
    "X_noboundary, T_noinitial = np.meshgrid(x_noboundary, t_noinitial) # X (99,255) = [[0.0245......2pi],\n",
    "                                                                   #               [0.0245......2pi],\n",
    "                                                                   #               [       ...     ]\n",
    "                                                                   #               [0.0245......2pi]], \n",
    "                                                                   #\n",
    "                                                                   # T (99,255) = [[0.01........0.01],\n",
    "                                                                   #               [0.02........0.02],\n",
    "                                                                   #               [       ...      ]\n",
    "                                                                   #               [1..............1]]\n",
    "X_star_noinitial_noboundary = np.hstack((X_noboundary.flatten()[:, None], T_noinitial.flatten()[:, None]))\n",
    "# X_star_noinitial_noboundary  (25245,2) = [[0.0245,0.01],\n",
    "#                                          [...,    0.01],\n",
    "#                                          [2pi,    0.01],\n",
    "#                                              ....\n",
    "#                                              ....\n",
    "#                                          [0.0245,   1],\n",
    "#                                          [...,      1],\n",
    "#                                          [2pi,      1]]   \n",
    "\n",
    "\n",
    "########## sample collocation points only from the interior (where the PDE is enforced)\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "        \n",
    "def sample_random(X_all, N):\n",
    "    \"\"\"Given an array of (x,t) points, sample N_pde points from this.\"\"\"\n",
    "    set_seed(0)\n",
    "\n",
    "    idx = np.random.choice(X_all.shape[0], N, replace=False)\n",
    "    X_sampled = X_all[idx, :]\n",
    "\n",
    "    return X_sampled\n",
    "\n",
    "X_f_train = sample_random(X_star_noinitial_noboundary, Nf) # (100,2): random (x,t) pairs \n",
    "\n",
    "\n",
    "########## u(x,t) exact solution ##########\n",
    "u_vals = reaction_solution(rho, nx, nt) # Exact solution (25600,)\n",
    "u_star = u_vals.reshape(-1, 1)          # Exact solution reshaped into (25600, 1) \n",
    "Exact = u_star.reshape(len(t), len(x))  # Exact on the (t,x) grid, (100,256)         ##########important##########\n",
    "\n",
    "########## Initial condition (t=0) ##########\n",
    "xx1 = np.hstack((X[0:1,:].T, T[0:1,:].T)) \n",
    "# T[0:1,:].shape = (1,256) and T[0:1,:].T.shape = (256,1) : [0,...,0].T,\n",
    "# X[0:1,:].shape = (1,256) and X[0:1,:].T.shape = (256,1) : [0,...,2pi].T, \n",
    "# xx1.shape = (256,2) : [[0,...,2pi].T,[0,...,0].T,]\n",
    "\n",
    "uu1 = Exact[0:1,:].T # (256,1), u(x,t=0),  uu1=[u(x1,t0),u(x2,t0),.....,u(x256,t0)].T\n",
    "uu2 = Exact[:,0:1]   # (100,1), u(x=0,t),  uu2=[u(x0,t0),u(x0,t1),.....,u(x0,t100)].T \n",
    "\n",
    "u0\n",
    "\n",
    "########## Boundary conditions (x=0 and x=2.pi) ##########\n",
    "# x=0\n",
    "x_bc_lb = np.array([x[0]]*t.shape[0]).reshape(-1, 1) # (100,1) : x_bc_lb = [0,...,0].T \n",
    "bc_lb = np.hstack((x_bc_ub, t)) # shape (100,2) boundary condition at x = 0,\n",
    "                                # bc_lb = [[0   0    ],\n",
    "                                #          [0   0.01 ],\n",
    "                                #             ...\n",
    "                                #          [0   1    ]]\n",
    "                \n",
    "# x=2.pi\n",
    "x_bc_ub = np.array([x[-1]]*t.shape[0]).reshape(-1, 1) # (100,1) : x_bc_ub = [2pi,...,2pi].T \n",
    "bc_ub = np.hstack((x_bc_ub, t)) # shape (100,2) boundary condition at x = 2pi, \n",
    "                                # bc_ub = [[2pi   0    ],\n",
    "                                #          [2pi   0.01 ],\n",
    "                                #             ...\n",
    "                                #          [2pi   1    ]]\n",
    "                \n",
    "\n",
    "########## TRAIN values ##########                \n",
    "u_train = uu1 # (256,1) : u(x,t=0)\n",
    "X_u_train = xx1 # (256,2) : (x,t=0) \n",
    "print(X_u_train.shape[-1])\n",
    "\n",
    "\n",
    "#For my own system\n",
    "predict_x = X_star.T\n",
    "\n",
    "train_x=  xx1.T # [[0, .... ,2pi]\n",
    "                #  [0,...0,,,,,0]]   #(2,256) , m=256\n",
    "    \n",
    "train_y= Exact[0:1,:] # (1,256)\n",
    "\n",
    "train_bc_L= bc_lb.T  # [[ 0,0,.......,0]\n",
    "                    #  [ 0, 0.01,....1 ]]  # (2,100), m=100\n",
    "    \n",
    "train_bc_R= bc_ub.T # [[2pi, 2pi, ...., 2pi]\n",
    "                    #  [0, 0.01, ....., 1  ]] # (2,100), m=100\n",
    "    \n",
    "train_x_rdn = X_f_train.T # [[.....x........]\n",
    "                          #  [......t...... ]]\n",
    "print(X_u_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bac3b6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z, target):\n",
    "    \"\"\"\n",
    "    sigmoid activation function\n",
    "    \n",
    "    Arguments:\n",
    "    Z      -- numpy array of any shape\n",
    "    target -- str: 'forward', 'first_derivative' or 'second_derivative'\n",
    "    \n",
    "    Returns:\n",
    "    A     -- output of sigmoid(z), sigmoid'(z), or sigmoid''(z). Same shape as Z\n",
    "    cache -- returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "    \n",
    "    if target = 'forward':\n",
    "        A = 1/(1+np.exp(-Z))\n",
    "    \n",
    "    elif target = 'first_derivative':\n",
    "        A = (np.exp(-Z))/(1+np.exp(-Z))**2\n",
    "    \n",
    "    elif target = 'second_derivative':\n",
    "        A = 2 * (np.exp(-2*Z))/(1+np.exp(-Z))**3 - (np.exp(-Z))/(1+np.exp(-Z))**2\n",
    "        \n",
    "    assert(A.shape == Z.shape)\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "    \n",
    "\n",
    "def relu(Z, target):\n",
    "    \"\"\"\n",
    "    RELU activation function\n",
    "\n",
    "    Arguments:\n",
    "    Z -- numpy array of any shape\n",
    "    target -- str: 'forward', 'first_derivative' or 'second_derivative'\n",
    "\n",
    "    Returns:\n",
    "    A -- output of relu(z), relu'(z), or relu''(z). Same shape as Z\n",
    "    cache -- returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "    \n",
    "    if target = 'forward':\n",
    "        A = np.maximum(0,Z)\n",
    "        \n",
    "    elif target = 'first_derivative':\n",
    "        A[Z>0]=1\n",
    "        A[Z<=0]=0\n",
    "        \n",
    "    elif target = 'second_derivative':\n",
    "        A = Z*0\n",
    "        \n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "    \n",
    "    cache = Z \n",
    "    return A, cache\n",
    "\n",
    "\n",
    "def tanh(Z):\n",
    "    \"\"\"\n",
    "    Tanh activation function\n",
    "\n",
    "    Arguments:\n",
    "    Z -- numpy array of any shape\n",
    "    target -- str: 'forward', 'first_derivative' or 'second_derivative'\n",
    "\n",
    "    Returns:\n",
    "    A -- output of tanh(z), tanh'(z), or tanh''(z). Same shape as Z\n",
    "    cache -- returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "\n",
    "    if target = 'forward':\n",
    "        A = (np.exp(Z) - np.exp(-Z))/(np.exp(Z) + np.exp(-Z))\n",
    "        \n",
    "    elif target = 'first_derivative':\n",
    "        A = 4/(np.exp(Z) + np.exp(-Z))**2\n",
    "        \n",
    "    elif target = 'second_derivative':\n",
    "        A = (8 * np.exp(-Z))/(np.exp(Z) + np.exp(-Z))**3 - (8 * np.exp(Z))/(np.exp(Z) + np.exp(-Z))**3\n",
    "        \n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "    \n",
    "    cache = Z \n",
    "    return A, cache\n",
    "\n",
    "\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single SIGMOID unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "\"\"\" tanh \"\"\"\n",
    "\n",
    "\n",
    "def tanh_backward(dA, cache):\n",
    "    \n",
    "    Z = cache\n",
    "    \n",
    "    s = (np.exp(Z) - np.exp(-Z))/(np.exp(Z) + np.exp(-Z))\n",
    "    dZ = dA * (1 - s**2)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    " \n",
    "\n",
    "\n",
    "def initialize_parameters(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)          # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1]) #*0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = W.dot(A) + b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "        \n",
    "    elif activation == \"tanh\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = tanh(Z)\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache\n",
    "\n",
    "def L_model_forward(X, parameters,bulk_activation):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)\n",
    "                the cache of linear_sigmoid_forward() (there is one, indexed L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation = bulk_activation)\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation = \"tanh\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    #cost = (1./m) * (-np.dot(Y,np.log(AL).T) - np.dot(1-Y, np.log(1-AL).T))\n",
    "    cost = (1/m)*np.sum((AL - Y) ** 2)\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = 1./m * np.dot(dZ,A_prev.T)\n",
    "    db = 1./m * np.sum(dZ, axis = 1, keepdims = True)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    elif activation == \"tanh\":\n",
    "        dZ = tanh_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def L_model_backward(AL, Y, caches, bulk_activation, loss):\n",
    "\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    if loss == \"binary\":\n",
    "        dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) \n",
    "    \n",
    "    elif loss == \"mean\":\n",
    "        dAL = 2 * (AL - Y)\n",
    "    \n",
    "    elif loss=='pde':\n",
    "        dAL=1\n",
    "        \n",
    "    else:\n",
    "        quit()\n",
    "        \n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"AL, Y, caches\". Outputs: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation = \"tanh\")\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache, activation = bulk_activation)\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "        \n",
    "    return parameters\n",
    "\n",
    "def predict(X, y, parameters):\n",
    "    \"\"\"\n",
    "    This function is used to predict the results of a  L-layer neural network.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data set of examples you would like to label\n",
    "    parameters -- parameters of the trained model\n",
    "    \n",
    "    Returns:\n",
    "    p -- predictions for the given dataset X\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2 # number of layers in the neural network\n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    # Forward propagation\n",
    "    probas, caches = L_model_forward(X, parameters)\n",
    "\n",
    "    \n",
    "    # convert probas to 0/1 predictions\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "    \n",
    "    #print results\n",
    "    #print (\"predictions: \" + str(p))\n",
    "    #print (\"true labels: \" + str(y))\n",
    "    print(\"Accuracy: \"  + str(np.sum((p == y)/m)))\n",
    "        \n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "139dfb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONSTANTS ###\n",
    "layers_dims = [2, 50, 50, 50, 1] #  4-layer model\n",
    "\n",
    "# GRADED FUNCTION: L_layer_model\n",
    "\n",
    "def L_layer_model_separate(X, Y, bcL, bcR, xrdn, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False, bulk_activation='relu'):#lr was 0.009\n",
    "    \n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    \n",
    "    parameters = initialize_parameters(layers_dims)\n",
    "    parametersBC = initialize_parameters(layers_dims)\n",
    "    parametersPDE = initialize_parameters(layers_dims)\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        \n",
    "        AL, caches = L_model_forward(X, parameters, bulk_activation)\n",
    "        ALL, cachesL = L_model_forward(bcL, parametersBC, bulk_activation)\n",
    "        ALR, cachesR = L_model_forward(bcR, parametersBC, bulk_activation)\n",
    "        ALPDE, cachesPDE = L_model_forward(xrdn, parametersPDE, bulk_activation)\n",
    "        \n",
    "        # PDE\n",
    "        gradsPDE = L_model_backward(ALPDE, ALPDE, cachesPDE, bulk_activation, 'pde')\n",
    "        u_t = gradsPDE[\"dA\" + str(0)]\n",
    "        f = u_t[1,:] - rho*ALPDE + rho*ALPDE**2\n",
    "        m = f.shape[1]\n",
    "        A0 = np.zeros(m)\n",
    "        \n",
    "        \n",
    "        cost = compute_cost(AL, Y)\n",
    "        costBC = compute_cost(ALL, ALR)\n",
    "        costPDE = compute_cost(f, 0)\n",
    "        \n",
    "        grads = L_model_backward(AL, Y, caches, bulk_activation, 'mean')\n",
    "        gradsL = L_model_backward(ALL, ALR, cachesL, bulk_activation, 'mean')\n",
    "        gradsR = L_model_backward(ALR, ALL, cachesR, bulk_activation, 'mean')\n",
    "        \n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        parametersBC = update_parameters(parametersBC, gradsL, learning_rate)\n",
    "        parametersBC = update_parameters(parametersBC, gradsR, learning_rate)\n",
    "      \n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"cost and costBC after iteration %i: %f %f\" %(i, cost, costBC))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(costBC)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost2')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "57a1aca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost and costBC after iteration 0: 0.143654 0.162582\n",
      "cost and costBC after iteration 100: 0.128511 0.008808\n",
      "cost and costBC after iteration 200: 0.124474 0.000409\n",
      "cost and costBC after iteration 300: 0.123223 0.000047\n",
      "cost and costBC after iteration 400: 0.122707 0.000016\n",
      "cost and costBC after iteration 500: 0.122309 0.000010\n",
      "cost and costBC after iteration 600: 0.121423 0.000007\n",
      "cost and costBC after iteration 700: 0.118888 0.000006\n",
      "cost and costBC after iteration 800: 0.116708 0.000005\n",
      "cost and costBC after iteration 900: 0.110778 0.000005\n",
      "cost and costBC after iteration 1000: 0.107377 0.000004\n",
      "cost and costBC after iteration 1100: 0.103657 0.000004\n",
      "cost and costBC after iteration 1200: 0.099525 0.000004\n",
      "cost and costBC after iteration 1300: 0.094934 0.000003\n",
      "cost and costBC after iteration 1400: 0.089850 0.000003\n",
      "cost and costBC after iteration 1500: 0.085076 0.000003\n",
      "cost and costBC after iteration 1600: 0.089540 0.000003\n",
      "cost and costBC after iteration 1700: 0.085090 0.000003\n",
      "cost and costBC after iteration 1800: 0.080793 0.000003\n",
      "cost and costBC after iteration 1900: 0.075907 0.000003\n",
      "cost and costBC after iteration 2000: 0.071399 0.000002\n",
      "cost and costBC after iteration 2100: 0.066363 0.000002\n",
      "cost and costBC after iteration 2200: 0.059781 0.000002\n",
      "cost and costBC after iteration 2300: 0.056344 0.000002\n",
      "cost and costBC after iteration 2400: 0.047449 0.000002\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = L_layer_model_separate(train_x, train_y, train_bc_L, train_bc_R, train_x_rdn, layers_dims, num_iterations = 2500, print_cost = True, bulk_activation='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "78d13fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONSTANTS ###\n",
    "layers_dims = [2, 50, 50, 50, 1] #  4-layer model\n",
    "\n",
    "# GRADED FUNCTION: L_layer_model\n",
    "\n",
    "def L_layer_model_together(X, Y, bcL, bcR, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False, bulk_activation='relu'):#lr was 0.009\n",
    "    \n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    \n",
    "    parameters = initialize_parameters(layers_dims)\n",
    "    parametersBC = initialize_parameters(layers_dims)\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        \n",
    "        AL, caches = L_model_forward(X, parameters, bulk_activation)\n",
    "        ALL, cachesL = L_model_forward(bcL, parameters, bulk_activation)\n",
    "        ALR, cachesR = L_model_forward(bcR, parameters, bulk_activation)\n",
    "        \n",
    "        \n",
    "        cost = compute_cost(AL, Y)\n",
    "        costBC = compute_cost(ALL, ALR)\n",
    "        costT = cost + costBC\n",
    "        \n",
    "        grads = L_model_backward(AL, Y, caches, bulk_activation)\n",
    "        gradsL = L_model_backward(ALL, ALR, cachesL, bulk_activation)\n",
    "        gradsR = L_model_backward(ALR, ALL, cachesR, bulk_activation)\n",
    "        \n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        parameters = update_parameters(parameters, gradsL, learning_rate)\n",
    "        parameters = update_parameters(parameters, gradsR, learning_rate)\n",
    "      \n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"cost and costBC after iteration %i: %f\" %(i, costT))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(costT)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost2')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "77b4b686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost and costBC after iteration 0: 0.306236\n",
      "cost and costBC after iteration 100: 0.147308\n",
      "cost and costBC after iteration 200: 0.126273\n",
      "cost and costBC after iteration 300: 0.124639\n",
      "cost and costBC after iteration 400: 0.123638\n",
      "cost and costBC after iteration 500: 0.123627\n",
      "cost and costBC after iteration 600: 0.122887\n",
      "cost and costBC after iteration 700: 0.122591\n",
      "cost and costBC after iteration 800: 0.121607\n",
      "cost and costBC after iteration 900: 0.120996\n",
      "cost and costBC after iteration 1000: 0.120588\n",
      "cost and costBC after iteration 1100: 0.119196\n",
      "cost and costBC after iteration 1200: 0.117668\n",
      "cost and costBC after iteration 1300: 0.115074\n",
      "cost and costBC after iteration 1400: 0.112439\n",
      "cost and costBC after iteration 1500: 0.110323\n",
      "cost and costBC after iteration 1600: 0.107479\n",
      "cost and costBC after iteration 1700: 0.105768\n",
      "cost and costBC after iteration 1800: 0.103258\n",
      "cost and costBC after iteration 1900: 0.100195\n",
      "cost and costBC after iteration 2000: 0.095757\n",
      "cost and costBC after iteration 2100: 0.092752\n",
      "cost and costBC after iteration 2200: 0.091861\n",
      "cost and costBC after iteration 2300: 0.084842\n",
      "cost and costBC after iteration 2400: 0.080748\n",
      "cost and costBC after iteration 2500: 0.076405\n",
      "cost and costBC after iteration 2600: 0.071777\n",
      "cost and costBC after iteration 2700: 0.067007\n",
      "cost and costBC after iteration 2800: 0.063436\n",
      "cost and costBC after iteration 2900: 0.060262\n",
      "cost and costBC after iteration 3000: 0.056571\n",
      "cost and costBC after iteration 3100: 0.052850\n",
      "cost and costBC after iteration 3200: 0.049044\n",
      "cost and costBC after iteration 3300: 0.046342\n",
      "cost and costBC after iteration 3400: 0.043863\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = L_layer_model_together(train_x, train_y, train_bc_L, train_bc_R, layers_dims, num_iterations = 3500, print_cost = True, bulk_activation='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dee59243",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "nx=256\n",
    "nt=100\n",
    "x = np.linspace(0, 2*np.pi, nx, endpoint=False).reshape(-1, 1) # (256,1): [0,...,2pi].T\n",
    "x_noinit = x[1:]                                               # (255,1): [0.0245,...,2pi].T \n",
    "t = np.linspace(0, 1, nt).reshape(-1, 1) # (100,1): [0,...,1].T\n",
    "t_noinit = t[1:]                         # (99,1):  [0.01,...,1].T\n",
    "\n",
    "\n",
    "# grid parameters with initial condition\n",
    "xgrid, tgrid = np.meshgrid(x, t) # xgrid (100,256) = [[0......2pi],\n",
    "                                 #                    [0......2pi],\n",
    "                                 #                    [   ...    ]\n",
    "                                 #                    [0......2pi]], \n",
    "                                 #\n",
    "                                 # tgrid (100,256) = [[0........0],\n",
    "                                 #                    [0.01..0.01],\n",
    "                                 #                    [    ...   ]\n",
    "                                 #                    [1........1]]\n",
    "X_grid_flatten = np.hstack((xgrid.flatten()[:, None], tgrid.flatten()[:, None])) \n",
    "# X_flatten (25600,2) = [[0,     0],\n",
    "#                        [0.0245,0],\n",
    "#                        [...,   0],\n",
    "#                        [2pi,   0],\n",
    "#                           ....\n",
    "#                           ....\n",
    "#                        [0,     1]\n",
    "#                        [0.0245,1],\n",
    "#                        [...,   1],\n",
    "#                        [2pi,   1]]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cc60793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.02454369, 0.04908739, ..., 6.20955423, 6.23409792,\n",
       "        6.25864161],\n",
       "       [0.        , 0.        , 0.        , ..., 1.        , 1.        ,\n",
       "        1.        ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_grid_flatten.T "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1524f925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.08439696, -0.03464089,  0.1429124 ],\n",
       "       [ 0.03159791, -0.04760369,  0.13350452]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randn(2,3)*0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31d8a32d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.10500052,  0.33146537,  0.36958305],\n",
       "       [ 0.84173015, -0.24411527, -0.10219934]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randn(2, 3) / np.sqrt(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a127f689",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
