{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f4822da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Physics informed neural networks\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Reaction function\n",
    "def reaction(u0, rho, dt):\n",
    "    \"\"\" solution of du/dt = rho*u*(1-u) with u0 = u(x,t=0) \"\"\"\n",
    "    numerator = u0 * np.exp(rho * dt)\n",
    "    denominator = u0 * np.exp(rho * dt) + 1 - u0\n",
    "    u = numerator / denominator\n",
    "    return u\n",
    "\n",
    "def reaction_solution(rho, x, tgrid):\n",
    "    \"\"\" discrete solution of du/dt = rho*u*(1-u) with u0 = u(x,t=0) \"\"\"\n",
    "    \n",
    "    # initial value: u0 = u(x,t=0)\n",
    "    u0 = lambda x: np.exp(-np.power((x - np.pi)/(np.pi/4), 2.)/2.) # + np.random.uniform(-0.05,0.05,256)\n",
    "    u0 = u0(x).squeeze() # (256,) : u(0),...,u(2pi)\n",
    "    \n",
    "    # function\n",
    "    u = reaction(u0, rho, tgrid) # (100,256)\n",
    "    u = u.flatten() # (25600,)\n",
    "    return u\n",
    "\n",
    "\n",
    "\n",
    "# number of points\n",
    "nx=256\n",
    "nt=100\n",
    "\n",
    "# pde parameters\n",
    "rho=1\n",
    "\n",
    "# input parameters\n",
    "x = np.linspace(0, 2*np.pi, nx, endpoint=False).reshape(-1, 1) # (256,1): [0,...,2pi].T\n",
    "x_noinit = x[1:]                                               # (255,1): [0.0245,...,2pi].T \n",
    "t = np.linspace(0, 1, nt).reshape(-1, 1) # (100,1): [0,...,1].T\n",
    "t_noinit = t[1:]                         # (99,1):  [0.01,...,1].T\n",
    "\n",
    "\n",
    "# grid parameters with initial condition\n",
    "xgrid, tgrid = np.meshgrid(x, t) # xgrid (100,256) = [[0......2pi],\n",
    "                                 #                    [0......2pi],\n",
    "                                 #                    [   ...    ]\n",
    "                                 #                    [0......2pi]], \n",
    "                                 #\n",
    "                                 # tgrid (100,256) = [[0........0],\n",
    "                                 #                    [0.01..0.01],\n",
    "                                 #                    [    ...   ]\n",
    "                                 #                    [1........1]]\n",
    "X_grid_flatten = np.hstack((xgrid.flatten()[:, None], tgrid.flatten()[:, None])) \n",
    "# X_flatten (25600,2) = [[0,     0],\n",
    "#                        [0.0245,0],\n",
    "#                        [...,   0],\n",
    "#                        [2pi,   0],\n",
    "#                           ....\n",
    "#                           ....\n",
    "#                        [0,     1]\n",
    "#                        [0.0245,1],\n",
    "#                        [...,   1],\n",
    "#                        [2pi,   1]]  \n",
    "\n",
    "\n",
    "# grid parameters without initial conditions\n",
    "xgrid_noinit, tgrid_noinit = np.meshgrid(x_noinit, t_noinit) \n",
    "# xgrid_noinit (99,255) = [[0.0245......2pi],\n",
    "#                          [0.0245......2pi],\n",
    "#                          [       ...     ]\n",
    "#                          [0.0245......2pi]], \n",
    "#\n",
    "# tgrid_noinit (99,255) = [[0.01........0.01],\n",
    "#                          [0.02........0.02],\n",
    "#                          [       ...      ]\n",
    "#                          [1..............1]]\n",
    "\n",
    "X_grid_flatten_noinit = np.hstack((xgrid_noinit.flatten()[:, None], tgrid_noinit.flatten()[:, None]))\n",
    "# X_flatten_noinit  (25245,2) = [[0.0245,0.01],\n",
    "#                                [...,    0.01],\n",
    "#                                [2pi,    0.01],\n",
    "#                                     ....\n",
    "#                                     ....\n",
    "#                                [0.0245,   1],\n",
    "#                                [...,      1],\n",
    "#                                [2pi,      1]]   \n",
    "\n",
    "\n",
    "#############################################\n",
    "##########   PDE Exact Solution    ##########\n",
    "#############################################\n",
    "\n",
    "u_values = reaction_solution(rho, x, tgrid).reshape(-1, 1) # (25600,1) \n",
    "ugrid = u_values.reshape(len(t), len(x))  #  (100,256) : (t,x) grid\n",
    "\n",
    "\n",
    "#############################################\n",
    "########## Initial condition (t=0) ##########\n",
    "#############################################\n",
    "\n",
    "N0 = x.shape[0] # number of samples\n",
    "\n",
    "t0 = np.array([t[0]]*N0).reshape(-1, 1)  # (256,1) : [0,...,0].T \n",
    "X_0 = np.hstack((x, t0))                 # (256,2) : [[0,...,2pi].T,[0,...,0].T] \n",
    "Y_0 = ugrid[0:1,:].T                     # (256,1) : [u(x1,t=0),u(x2,0),.....,u(x256,0)].T\n",
    "\n",
    "\n",
    "############################################\n",
    "##########  Boundary conditions  ###########\n",
    "############################################\n",
    "\n",
    "Nbc = t.shape[0] # number of samples\n",
    "\n",
    "xL = np.array([x[0]]*Nbc).reshape(-1, 1) # (100,1) : [0,...,0].T \n",
    "X_bc_L = np.hstack((xL, t))              # (100,2) : [[0   0    ],\n",
    "                                         #            [0   0.01 ],\n",
    "                                         #               ...\n",
    "                                         #            [0   1    ]]\n",
    "                \n",
    "                \n",
    "xR = np.array([x[-1]]*Nbc).reshape(-1, 1) # (100,1) : [2pi,...,2pi].T \n",
    "X_bc_R = np.hstack((xR, t))               # (100,2) : [[2pi   0    ],\n",
    "                                          #            [2pi   0.01 ],\n",
    "                                          #                ...\n",
    "                                          #            [2pi   1    ]]\n",
    "                \n",
    "\n",
    "#############################################\n",
    "#############   PDE condition    ############\n",
    "#############################################\n",
    "\n",
    "Npde = t.shape[0] # number of samples\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"Set a fixed random configuration\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "        \n",
    "def sample_random(X_all, N):\n",
    "    \"\"\"Given an array of (x,t) points, sample Npde points from this.\"\"\"\n",
    "    set_seed(0)\n",
    "\n",
    "    idx = np.random.choice(X_all.shape[0], N, replace=False)\n",
    "    X_sampled = X_all[idx, :]\n",
    "\n",
    "    return X_sampled\n",
    "\n",
    "X_pde = sample_random(X_grid_flatten_noinit, Npde) # (100,2): random (x,t) pairs \n",
    "\n",
    "\n",
    "############   TRAINING   ############ \n",
    "\n",
    "#initial conditions \n",
    "train_X0 = X_0.T # (2,256): [[0, .... ,2pi]\n",
    "                 #           [0,...0,,,,,0]]   \n",
    "    \n",
    "train_Y0 = Y_0.T # (1,256)\n",
    "\n",
    "#boundary conditions\n",
    "train_Xbc_L = X_bc_L.T # (2,100): [[ 0,0,.......,0]\n",
    "                       #            [ 0, 0.01,....1 ]] \n",
    "    \n",
    "train_Xbc_R = X_bc_R.T # (2,100):  [[2pi, 2pi, ...., 2pi]\n",
    "                       #            [0, 0.01, ....., 1  ]] \n",
    "    \n",
    "#pde condition\n",
    "    \n",
    "train_Xpde = X_pde.T # [[.....x........]\n",
    "                    #  [......t...... ]]\n",
    "\n",
    "\n",
    "############## PREDICTING  ###############\n",
    "\n",
    "predict_X = X_grid_flatten.T # (2, 25600)\n",
    "predict_Y = u_values.T # (1, 25600)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc9f777e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################################\n",
    "########################################   Neural Network Functions   #############################################\n",
    "###################################################################################################################\n",
    "\n",
    "\n",
    "##################################################\n",
    "########       Activation Functions        #######\n",
    "##################################################\n",
    "\n",
    "def relu(Z, target):\n",
    "    \"\"\"\n",
    "    RELU activation function\n",
    "\n",
    "    Arguments:\n",
    "    Z -- numpy array of any shape\n",
    "    target -- str: 'forward', 'first_derivative' or 'second_derivative'\n",
    "\n",
    "    Returns:\n",
    "    A -- output of relu(z), relu'(z), or relu''(z). Same shape as Z\n",
    "    cache -- returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "    \n",
    "    if target == 'forward':\n",
    "        A = np.maximum(0,Z)\n",
    "        \n",
    "    elif target == 'first_derivative':\n",
    "        A = np.array(Z, copy=True)\n",
    "        A[Z>0]=1\n",
    "        A[Z<=0]=0\n",
    "        \n",
    "    elif target == 'second_derivative':\n",
    "        A = Z*0 \n",
    "        \n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "    \n",
    "    cache = Z \n",
    "    return A, cache\n",
    "\n",
    "\n",
    "def sigmoid(Z, target):\n",
    "    \"\"\"\n",
    "    sigmoid activation function\n",
    "    \n",
    "    Arguments:\n",
    "    Z      -- numpy array of any shape\n",
    "    target -- str: 'forward', 'first_derivative' or 'second_derivative'\n",
    "    \n",
    "    Returns:\n",
    "    A     -- output of sigmoid(z), sigmoid'(z), or sigmoid''(z). Same shape as Z\n",
    "    cache -- returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "    \n",
    "    if target == 'forward':\n",
    "        A = 1/(1+np.exp(-Z))\n",
    "    \n",
    "    elif target == 'first_derivative':\n",
    "        A = (np.exp(-Z))/(1+np.exp(-Z))**2\n",
    "    \n",
    "    elif target == 'second_derivative':\n",
    "        A = 2 * (np.exp(-2*Z))/(1+np.exp(-Z))**3 - (np.exp(-Z))/(1+np.exp(-Z))**2\n",
    "        \n",
    "    assert(A.shape == Z.shape)\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "\n",
    "def tanh(Z, target):\n",
    "    \"\"\"\n",
    "    Tanh activation function\n",
    "\n",
    "    Arguments:\n",
    "    Z -- numpy array of any shape\n",
    "    target -- str: 'forward', 'first_derivative' or 'second_derivative'\n",
    "\n",
    "    Returns:\n",
    "    A -- output of tanh(z), tanh'(z), or tanh''(z). Same shape as Z\n",
    "    cache -- returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "\n",
    "    if target == 'forward':\n",
    "        A = (np.exp(Z) - np.exp(-Z))/(np.exp(Z) + np.exp(-Z))\n",
    "        \n",
    "    elif target == 'first_derivative':\n",
    "        A = 4/(np.exp(Z) + np.exp(-Z))**2\n",
    "        \n",
    "    elif target == 'second_derivative':\n",
    "        A = (8 * np.exp(-Z))/(np.exp(Z) + np.exp(-Z))**3 - (8 * np.exp(Z))/(np.exp(Z) + np.exp(-Z))**3\n",
    "        \n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "    \n",
    "    cache = Z \n",
    "    return A, cache\n",
    "\n",
    "\n",
    "def identity(Z, target):\n",
    "    \"\"\"\n",
    "    Identity activation function\n",
    "\n",
    "    Arguments:\n",
    "    Z -- numpy array of any shape\n",
    "    target -- str: 'forward', 'first_derivative' or 'second_derivative'\n",
    "\n",
    "    Returns:\n",
    "    A -- output of identity(z), identity'(z), or identity''(z). Same shape as Z\n",
    "    cache -- returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "\n",
    "    if target == 'forward':\n",
    "        A = Z\n",
    "        \n",
    "    elif target == 'first_derivative':\n",
    "        temp = np.zeros((Z.shape[0],Z.shape[1]))\n",
    "        temp[:] = 1\n",
    "        A = temp\n",
    "        \n",
    "    elif target == 'second_derivative':\n",
    "        temp = np.zeros((Z.shape[0],Z.shape[1]))\n",
    "        A = temp\n",
    "        \n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "    \n",
    "    cache = Z \n",
    "    return A, cache\n",
    "\n",
    "\n",
    "def relu_backward(dA, cache, target):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "    target -- str: 'forward', 'pde_first_order' or 'pde_second_order'\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    \n",
    "    if target == 'backward':\n",
    "        dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "        dZ[Z <= 0] = 0 # --> dA * ds/dZ\n",
    "        \n",
    "    elif target == 'pde_first_order':\n",
    "        dZ = dA * 0 # --> dA * d2s/dZ2\n",
    "    \n",
    "    elif target == 'pde_second_order':\n",
    "        dZ = dA * 0 # --> dA * d3s/dZ3\n",
    "    \n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def sigmoid_backward(dA, cache, target):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single SIGMOID unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "    target -- str: 'forward', 'pde_first_order' or 'pde_second_order'\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "\n",
    "    if target == 'backward':\n",
    "        s = 1/(1+np.exp(-Z)) # --> ds/dZ = s*(1-s)\n",
    "        dZ = dA * s * (1-s)\n",
    "        \n",
    "    elif target == 'pde_first_order':\n",
    "        s = 1/(1+np.exp(-Z)) # --> d2s/dZ2 = (1 - 2*s) * (s-s**2)\n",
    "        dZ = dA * (1 - 2*s) * (s-s**2)\n",
    "    \n",
    "    elif target == 'pde_second_order':\n",
    "        s = 1/(1+np.exp(-Z)) # --> d3s/dZ3 = (1 - 6*s + 6*s**2) * (s-s**2)\n",
    "        dZ = dA * (1 - 6*s + 6*s**2) * (s-s**2)\n",
    "          \n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "\n",
    "def tanh_backward(dA, cache, target):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single TANH unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "    target -- str: 'forward', 'pde_first_order' or 'pde_second_order'\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    \n",
    "    if target == 'backward':\n",
    "        s = (np.exp(Z) - np.exp(-Z))/(np.exp(Z) + np.exp(-Z)) # --> ds/dZ = (1 - s**2) \n",
    "        dZ = dA * (1 - s**2)\n",
    "        \n",
    "    elif target == 'pde_first_order':\n",
    "        s = (np.exp(Z) - np.exp(-Z))/(np.exp(Z) + np.exp(-Z)) # --> d2s/dZ2 = -2s(1 - s**2) \n",
    "        dZ = dA * -2*s*(1 - s**2)\n",
    "    \n",
    "    elif target == 'pde_second_order':\n",
    "        s = (np.exp(Z) - np.exp(-Z))/(np.exp(Z) + np.exp(-Z)) # --> d3s/dZ3 = -2*(1- s**2) + 6*s**2*(1-s**2)\n",
    "        dZ = dA * (-2 + 6*s**2) * (1-s**2)\n",
    "        \n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "\n",
    "def identity_backward(dA, cache, target):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single IDENTITY unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "    target -- str: 'forward', 'pde_first_order' or 'pde_second_order'\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "\n",
    "    \n",
    "    if target == 'backward':\n",
    "        s = Z # --> ds/dZ = Z/Z\n",
    "        temp = np.zeros((Z.shape[0],Z.shape[1]))\n",
    "        temp[:] = 1\n",
    "        dZ = dA * temp\n",
    "        \n",
    "    elif target == 'pde_first_order':\n",
    "        s = Z # --> d2s/dZ2 = 0\n",
    "        temp = np.zeros((Z.shape[0],Z.shape[1]))\n",
    "        dZ = dA * temp\n",
    "    \n",
    "    elif target == 'pde_second_order':\n",
    "        s = Z # --> d3s/dZ3 = 0\n",
    "        temp = np.zeros((Z.shape[0],Z.shape[1]))\n",
    "        dZ = dA * temp\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "##################################################\n",
    "########  Initialize Parameters Function   #######\n",
    "##################################################\n",
    "\n",
    "def initialize_parameters(layers_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layers_dims -- list containing the dimensions of each layer in the network (including the input layer)\n",
    "    \n",
    "    Returns:\n",
    "    parameters --  dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                   Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                   bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(2)\n",
    "    parameters = {}\n",
    "\n",
    "    for l in range(1, len(layers_dims)):\n",
    "        parameters['W' + str(l)] = np.random.randn(layers_dims[l], layers_dims[l-1]) / np.sqrt(layers_dims[l-1]) \n",
    "        parameters['b' + str(l)] = np.random.randn(layers_dims[l], 1) * 0.01\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layers_dims[l], layers_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layers_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters\n",
    "\n",
    "\n",
    "##################################################\n",
    "########   Forward Propagation Functions   #######\n",
    "##################################################\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = W.dot(A) + b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\", \"relu\" or \"tanh\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a tuple containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z, \"forward\")\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z , \"forward\")\n",
    "        \n",
    "    elif activation == \"tanh\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = tanh(Z, \"forward\")\n",
    "    \n",
    "    elif activation == \"identity\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = identity(Z, \"forward\")\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache\n",
    "\n",
    "def L_model_forward(X, parameters, bulk_activation):\n",
    "    \"\"\"\n",
    "    Implement: \n",
    "    forward propagation for the bulk LINEAR->ACTIVATION layers\n",
    "    \n",
    "    forward propagation for the last LINEAR->ACTIVATION layer\n",
    "    *In PINNs the activation function of the last layer is the identity function \n",
    "     because the output is a real number\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters()\n",
    "    bulk_activation -- activation function in the bulk\n",
    "    \n",
    "    Returns:\n",
    "    AL -- Ouput of the neural network. last value of the last LINEAR->ACTIVATION layer\n",
    "    caches -- list of caches, e.g., caches[0] = (linear_cache, activation cache) = ((A0,W1,b1),Z1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2   # number of layers in the neural network (without the input layer)\n",
    "    \n",
    "    # Bulk layers\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation = bulk_activation)\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # Last layer\n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation = \"identity\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches\n",
    "\n",
    "##################################################\n",
    "#######           Cost Function            #######\n",
    "##################################################\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function.\n",
    "\n",
    "    Arguments:\n",
    "    AL -- output of the neural network, shape (1, number of examples)\n",
    "    Y --  true solution vector, e.g. exact solution of the pde\n",
    "\n",
    "    Returns:\n",
    "    cost -- mean squared error or cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = AL.shape[1]\n",
    "\n",
    "    #cost = (1./m) * (-np.dot(Y,np.log(AL).T) - np.dot(1-Y, np.log(1-AL).T))\n",
    "    cost = (1/m)*np.sum((AL - Y) ** 2)\n",
    "    \n",
    "    cost = np.squeeze(cost) # To make sure your cost's shape is what we expect (e.g. it turns [[17]] into 17)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost\n",
    "\n",
    "#################################################\n",
    "#######   Autograd Propagation Functions ########\n",
    "#################################################\n",
    "\n",
    "\n",
    "###### Reverse: last layer to first layer #######\n",
    "def linear_autogradL(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of autograd propagation (reverse) for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of AL with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of AL with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    \n",
    "    return dA_prev\n",
    "\n",
    "def linear_activation_autogradL(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the autograd propagation (reverse) for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\", \"relu\", \"tanh\", ...\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of AL with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache, target=\"backward\")\n",
    "        dA_prev = linear_autogradL(dZ, linear_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache, target=\"backward\")\n",
    "        dA_prev = linear_autogradL(dZ, linear_cache)\n",
    "        \n",
    "    elif activation == \"tanh\":\n",
    "        dZ = tanh_backward(dA, activation_cache, target=\"backward\")\n",
    "        dA_prev = linear_autogradL(dZ, linear_cache)\n",
    "    \n",
    "    elif activation == \"identity\":\n",
    "        dZ = identity_backward(dA, activation_cache, target=\"backward\")\n",
    "        dA_prev = linear_autogradL(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dZ\n",
    "\n",
    "\n",
    "###### Forward: first layer to last layer #######\n",
    "def linear_autogradR(dz, cache):\n",
    "\n",
    "    A_prev, W, b = cache\n",
    "    da_prev = np.dot(W,dz)\n",
    "    \n",
    "    return da_prev\n",
    "\n",
    "def linear_activation_autogradR(dz, cache, activation):\n",
    "\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        da_prev = linear_autogradR(dz,linear_cache)\n",
    "        dz = relu_backward(da_prev, activation_cache, target=\"backward\")\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        da_prev = linear_autogradR(dz,linear_cache)\n",
    "        dz = sigmoid_backward(da_prev, activation_cache, target=\"backward\")\n",
    "        \n",
    "    elif activation == \"tanh\":\n",
    "        da_prev = linear_autogradR(dz,linear_cache)\n",
    "        dz = tanh_backward(da_prev, activation_cache, target=\"backward\")\n",
    "            \n",
    "    elif activation == \"identity\":\n",
    "        da_prev = linear_autogradR(dz,linear_cache)\n",
    "        dz = identity_backward(da_prev, activation_cache, target=\"backward\")\n",
    "    \n",
    "    return da_prev, dz\n",
    "\n",
    "####### General: calls reverse and forward modes #######\n",
    "def L_model_autograd(dAL, caches, bulk_activation, mode):\n",
    "\n",
    "    if mode==\"reverse\":\n",
    "        autogradL = {}\n",
    "        L = len(caches) # the number of layers\n",
    "    \n",
    "        # Lth layer \n",
    "        # Inputs: \"caches\", \"bulk activation\". Outputs: \"grads[\"dAL\"]\n",
    "        autogradL[\"dA\" + str(L)] = dAL\n",
    "        current_cache = caches[L-1]\n",
    "        autogradL[\"dA\" + str(L-1)], autogradL[\"dZ\" + str(L)] = linear_activation_autogradL(autogradL[\"dA\" + str(L)], current_cache, activation = \"identity\")\n",
    "    \n",
    "        # Buk layers\n",
    "        for l in reversed(range(L-1)):\n",
    "            current_cache = caches[l]\n",
    "            dA_prev_temp, dZ_temp = linear_activation_autogradL(autogradL[\"dA\" + str(l + 1)], current_cache, activation = bulk_activation)\n",
    "            autogradL[\"dA\" + str(l)] = dA_prev_temp\n",
    "            autogradL[\"dZ\" + str(l+1)] = dZ_temp\n",
    "\n",
    "        return autogradL\n",
    "    \n",
    "    elif mode==\"forward\":\n",
    "        autogradR = {}\n",
    "        L = len(caches) # the number of layers\n",
    "        \n",
    "        linear_cache, activation_cache = caches[0]\n",
    "        A0, W1, b1 = linear_cache\n",
    "        temp = np.zeros((A0.shape[0],A0.shape[1]))\n",
    "        temp[1,:] = 1 \n",
    "        \n",
    "        # Bulk\n",
    "        autogradR[\"dz\" + str(0)] = temp \n",
    "        for l in range(L-1):\n",
    "            current_cache = caches[l]\n",
    "            autogradR[\"da\" + str(l)], autogradR[\"dz\" + str(l+1)] = linear_activation_autogradR(autogradR[\"dz\" + str(l)], current_cache, activation = bulk_activation)\n",
    "        \n",
    "        # Lth layer\n",
    "        current_cache = caches[L-1]\n",
    "        autogradR[\"da\" + str(L-1)], autogradR[\"dz\" + str(L)] = linear_activation_autogradR(autogradR[\"dz\" + str(L-1)], current_cache, activation = \"identity\")\n",
    "\n",
    "        return autogradR\n",
    "    \n",
    "    else:\n",
    "        quit()\n",
    "\n",
    "\n",
    "        \n",
    "##################################################\n",
    "#######   Backward Propagation Functions   #######\n",
    "##################################################\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = 1./m * np.dot(dZ,A_prev.T)\n",
    "    db = 1./m * np.sum(dZ, axis = 1, keepdims = True)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache, \"backward\")\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache, \"backward\")\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    elif activation == \"tanh\":\n",
    "        dZ = tanh_backward(dA, activation_cache, \"backward\")\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    elif activation == \"identity\":\n",
    "        dZ = identity_backward(dA, activation_cache, \"backward\")\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def L_model_backward(AL, Y, caches, bulk_activation, loss):\n",
    "\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    if loss == \"binary\":\n",
    "        dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) \n",
    "    \n",
    "    elif loss == \"mean\":\n",
    "        dAL = 2 * (AL - Y)\n",
    "        \n",
    "    else:\n",
    "        quit()\n",
    "        \n",
    "    \n",
    "    # Lth layer ([ACTIVATION FUNCTION] LAST LAYER -> LINEAR) gradients. \n",
    "    # Inputs: \"AL, Y, caches\". Outputs: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    current_cache = caches[L-1]\n",
    "    dA_prev_temp, dW_temp, db_temp = linear_activation_backward(dAL, current_cache, activation = \"identity\")\n",
    "    grads[\"dA\" + str(L-1)] = dA_prev_temp\n",
    "    grads[\"dW\" + str(L)] = dW_temp\n",
    "    grads[\"db\" + str(L)] = db_temp\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: ([ACTIVATION FUNCTION] BULK LAYERS -> LINEAR) gradients.\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache, activation = bulk_activation)\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads\n",
    "\n",
    "\n",
    "##################################################\n",
    "###### Backward Propagation Functions PDEs  ######\n",
    "##################################################\n",
    "def linear_backward_pde(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = 1./m * np.dot(dZ,A_prev.T)\n",
    "    db = 1./m * np.sum(dZ, axis = 1, keepdims = True)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def linear_activation_backward_pde(dA, cache, activation, target):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache, target)\n",
    "        dA_prev, dW, db = linear_backward_pde(dZ, linear_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache, target)\n",
    "        dA_prev, dW, db = linear_backward_pde(dZ, linear_cache)\n",
    "        \n",
    "    elif activation == \"tanh\":\n",
    "        dZ = tanh_backward(dA, activation_cache, target)\n",
    "        dA_prev, dW, db = linear_backward_pde(dZ, linear_cache)\n",
    "    \n",
    "    elif activation == \"identity\":\n",
    "        dZ = identity_backward(dA, activation_cache, target)\n",
    "        dA_prev, dW, db = linear_backward_pde(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "\n",
    "\n",
    "def L_model_backward_pde(AL, f, caches, bulk_activation, loss, term, rho):\n",
    "\n",
    "    gradsPDE = {}\n",
    "\n",
    "    L = len(caches) # the number of layers\n",
    "    m = f.shape[1]\n",
    "    \n",
    "    # Caculate dAL = dJ/daL\n",
    "    if loss == \"mean\":\n",
    "        \n",
    "        # PDE1 (first term) : du/dt\n",
    "        if term == \"pde1\":\n",
    "            dAL = 2 * f\n",
    "            \n",
    "        # PDE2 (second term): -rho*u\n",
    "        elif term == \"pde2\":    \n",
    "            dAL = -2 * rho * f\n",
    "            \n",
    "        # PDE3 (third term) : +rho*u**2    \n",
    "        elif term == \"pde3\":\n",
    "            dAL = 4 * rho * f * AL\n",
    "        else:\n",
    "            quit()\n",
    "            \n",
    "    else:\n",
    "        quit()\n",
    "    \n",
    "    \n",
    "    # PDE1 (first term)\n",
    "    if term == \"pde1\":\n",
    "        \n",
    "        # Compute the autograd (from right to left). Specific to dz/dt\n",
    "        autogradsR = L_model_autograd(1, caches, bulk_activation, \"forward\")\n",
    "        \n",
    "        # Compute the autograd (from left to right). General solution\n",
    "        autogradsL = L_model_autograd(dAL, caches, bulk_activation, \"reverse\")\n",
    "        \n",
    "        gradsPDE_w = {}\n",
    "        gradsPDE_g = {}\n",
    "\n",
    "        # gradsPDE_w.\n",
    "        for l in reversed(range(L)):\n",
    "            assert (autogradsL[\"dZ\"+str(l+1)].shape[1] == autogradsR[\"dz\"+str(l)].shape[1])\n",
    "            m = autogradsL[\"dZ\"+str(l+1)].shape[1]\n",
    "            gradsPDE_w[\"dW\" + str(l+1)] = 1./m * np.dot(autogradsL[\"dZ\"+str(l+1)], autogradsR[\"dz\"+str(l)].T)\n",
    "        \n",
    "             \n",
    "\n",
    "        #L=3\n",
    "        dAL = autogradsL[\"dA\"+str(L-1)] * autogradsR[\"da\"+str(L-2)]\n",
    "        current_cache = caches[L-2]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward_pde(dAL, current_cache, activation = bulk_activation, target=\"pde_first_order\")\n",
    "        gradsPDE_g[\"dA\" + str(L-2) + \"_g\" + str(L-1)] = dA_prev_temp\n",
    "        gradsPDE_g[\"dW\" + str(L-1) + \"_g\" + str(L-1)] = dW_temp\n",
    "        gradsPDE_g[\"db\" + str(L-1) + \"_g\" + str(L-1)] = db_temp\n",
    "    \n",
    "        for l in reversed(range(L-2)):\n",
    "            current_cache = caches[l]\n",
    "            dA_prev_temp, dW_temp, db_temp = linear_activation_backward_pde(gradsPDE_g[\"dA\" + str(l + 1) + \"_g\" + str(L-1)], current_cache, activation = bulk_activation, target=\"backward\")\n",
    "            gradsPDE_g[\"dA\" + str(l) + \"_g\" + str(L-1)] = dA_prev_temp\n",
    "            gradsPDE_g[\"dW\" + str(l + 1) + \"_g\" + str(L-1)] = dW_temp\n",
    "            gradsPDE_g[\"db\" + str(l + 1) + \"_g\" + str(L-1)] = db_temp\n",
    "        \n",
    "        #L=2\n",
    "        dAL = autogradsL[\"dA\"+str(L-2)] * autogradsR[\"da\"+str(L-3)]\n",
    "        current_cache = caches[L-3]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward_pde(dAL, current_cache, activation = bulk_activation, target=\"pde_first_order\")\n",
    "        gradsPDE_g[\"dA\" + str(L-3) + \"_g\" + str(L-2)] = dA_prev_temp\n",
    "        gradsPDE_g[\"dW\" + str(L-2) + \"_g\" + str(L-2)] = dW_temp\n",
    "        gradsPDE_g[\"db\" + str(L-2) + \"_g\" + str(L-2)] = db_temp\n",
    "    \n",
    "        for l in reversed(range(L-3)):\n",
    "            current_cache = caches[l]\n",
    "            dA_prev_temp, dW_temp, db_temp = linear_activation_backward_pde(gradsPDE_g[\"dA\" + str(l + 1) + \"_g\" + str(L-2)], current_cache, activation = bulk_activation, target=\"backward\")\n",
    "            gradsPDE_g[\"dA\" + str(l) + \"_g\" + str(L-2)] = dA_prev_temp\n",
    "            gradsPDE_g[\"dW\" + str(l + 1) + \"_g\" + str(L-2)] = dW_temp\n",
    "            gradsPDE_g[\"db\" + str(l + 1) + \"_g\" + str(L-2)] = db_temp\n",
    "        \n",
    "        \n",
    "        #L=1\n",
    "        dAL = autogradsL[\"dA\"+str(L-3)] * autogradsR[\"da\"+str(L-4)]\n",
    "        current_cache = caches[L-4]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward_pde(dAL, current_cache, activation = bulk_activation, target=\"pde_first_order\")\n",
    "        gradsPDE_g[\"dA\" + str(L-4) + \"_g\" + str(L-3)] = dA_prev_temp\n",
    "        gradsPDE_g[\"dW\" + str(L-3) + \"_g\" + str(L-3)] = dW_temp\n",
    "        gradsPDE_g[\"db\" + str(L-3) + \"_g\" + str(L-3)] = db_temp\n",
    "\n",
    "            \n",
    "\n",
    "        # TOTAL\n",
    "        gradsPDE[\"dW\" + str(4)] = gradsPDE_w[\"dW\" + str(4)]\n",
    "        gradsPDE[\"dW\" + str(3)] = gradsPDE_g[\"dW\" + str(3) + \"_g\" + str(3)] + gradsPDE_w[\"dW\" + str(3)]\n",
    "        gradsPDE[\"dW\" + str(2)] = gradsPDE_g[\"dW\" + str(2) + \"_g\" + str(3)] + gradsPDE_g[\"dW\" + str(2) + \"_g\" + str(2)] + gradsPDE_w[\"dW\" + str(2)]\n",
    "        gradsPDE[\"dW\" + str(1)] = gradsPDE_g[\"dW\" + str(1) + \"_g\" + str(3)] + gradsPDE_g[\"dW\" + str(1) + \"_g\" + str(2)] + gradsPDE_g[\"dW\" + str(1) + \"_g\" + str(1)] + gradsPDE_w[\"dW\" + str(1)]\n",
    "        \n",
    "        gradsPDE[\"db\" + str(4)] = 0\n",
    "        gradsPDE[\"db\" + str(3)] = gradsPDE_g[\"db\" + str(3) + \"_g\" + str(3)] \n",
    "        gradsPDE[\"db\" + str(2)] = gradsPDE_g[\"db\" + str(2) + \"_g\" + str(3)] + gradsPDE_g[\"db\" + str(2) + \"_g\" + str(2)] \n",
    "        gradsPDE[\"db\" + str(1)] = gradsPDE_g[\"db\" + str(1) + \"_g\" + str(3)] + gradsPDE_g[\"db\" + str(1) + \"_g\" + str(2)] + gradsPDE_g[\"db\" + str(1) + \"_g\" + str(1)]        \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    # PDE2 (second term)\n",
    "    elif term == \"pde2\": \n",
    "        \n",
    "        # Lth layer ([ACTIVATION FUNCTION] LAST LAYER -> LINEAR) gradients. \n",
    "        # Inputs: \"AL, caches\". Outputs: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "        current_cache = caches[L-1]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(dAL, current_cache, activation = \"identity\") # correct: no need of linear_activation_backward_pde\n",
    "        gradsPDE[\"dA\" + str(L-1)] = dA_prev_temp\n",
    "        gradsPDE[\"dW\" + str(L)] = dW_temp\n",
    "        gradsPDE[\"db\" + str(L)] = db_temp\n",
    "\n",
    "        for l in reversed(range(L-1)):\n",
    "            # lth layer: ([ACTIVATION FUNCTION] BULK LAYERS -> LINEAR) gradients.\n",
    "            current_cache = caches[l]\n",
    "            dA_prev_temp, dW_temp, db_temp = linear_activation_backward(gradsPDE[\"dA\" + str(l + 1)], current_cache, activation = bulk_activation) # correct: no need of linear_activation_backward_pde \n",
    "            gradsPDE[\"dA\" + str(l)] = dA_prev_temp\n",
    "            gradsPDE[\"dW\" + str(l + 1)] = dW_temp\n",
    "            gradsPDE[\"db\" + str(l + 1)] = db_temp\n",
    "    \n",
    "    \n",
    "    # PDE3 (third term)\n",
    "    elif term == \"pde3\": \n",
    "        \n",
    "        # Lth layer ([ACTIVATION FUNCTION] LAST LAYER -> LINEAR) gradients. \n",
    "        # Inputs: \"AL, caches\". Outputs: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "        current_cache = caches[L-1]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(dAL, current_cache, activation = \"identity\") # correct: no need of linear_activation_backward_pde\n",
    "        gradsPDE[\"dA\" + str(L-1)] = dA_prev_temp\n",
    "        gradsPDE[\"dW\" + str(L)] = dW_temp\n",
    "        gradsPDE[\"db\" + str(L)] = db_temp\n",
    "\n",
    "        for l in reversed(range(L-1)):\n",
    "            # lth layer: ([ACTIVATION FUNCTION] BULK LAYERS -> LINEAR) gradients.\n",
    "            current_cache = caches[l]\n",
    "            dA_prev_temp, dW_temp, db_temp = linear_activation_backward(gradsPDE[\"dA\" + str(l + 1)], current_cache, activation = bulk_activation) # correct: no need of linear_activation_backward_pde \n",
    "            gradsPDE[\"dA\" + str(l)] = dA_prev_temp\n",
    "            gradsPDE[\"dW\" + str(l + 1)] = dW_temp\n",
    "            gradsPDE[\"db\" + str(l + 1)] = db_temp\n",
    "    \n",
    "    else:\n",
    "        quit()\n",
    "\n",
    "    return gradsPDE\n",
    "\n",
    "\n",
    "\n",
    "##################################################\n",
    "#######  Gradient Descent Update Function  #######\n",
    "##################################################\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "        \n",
    "    return parameters\n",
    "\n",
    "def predict(X, y, parameters):\n",
    "    \"\"\"\n",
    "    This function is used to predict the results of a  L-layer neural network.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data set of examples you would like to label\n",
    "    parameters -- parameters of the trained model\n",
    "    \n",
    "    Returns:\n",
    "    p -- predictions for the given dataset X\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2 # number of layers in the neural network\n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    # Forward propagation\n",
    "    probas, caches = L_model_forward(X, parameters)\n",
    "\n",
    "    \n",
    "    # convert probas to 0/1 predictions\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "    \n",
    "    #print results\n",
    "    #print (\"predictions: \" + str(p))\n",
    "    #print (\"true labels: \" + str(y))\n",
    "    print(\"Accuracy: \"  + str(np.sum((p == y)/m)))\n",
    "        \n",
    "    return p\n",
    "\n",
    "##################################################\n",
    "#######                PDE                 #######\n",
    "##################################################\n",
    "\n",
    "def L_model_pde(AL,caches,bulk_activation):\n",
    "    \"\"\"\n",
    "    This function sets the pde \n",
    "    \n",
    "    Arguments:\n",
    "    AL --  output data of the neural network \n",
    "    caches -- list of the neural network variables, e.g., caches[0] = [[A0,W1,b1],Z1] \n",
    "    \n",
    "    Returns:\n",
    "    f -- pde expression considering the neural network output\n",
    "    \"\"\"    \n",
    "\n",
    "    ##### pde definition #####\n",
    "       \n",
    "    # du/dt - rho*u + rho*u**2 = 0\n",
    "    \n",
    "    # first-term\n",
    "    grads = L_model_autograd(1, caches, bulk_activation, \"reverse\")\n",
    "    grad_u = grads[\"dA\" + str(0)]\n",
    "    u_t = grad_u[1,:]\n",
    "    \n",
    "    ##### pde resolution #####\n",
    "    rho = 1\n",
    "    f = u_t - rho*AL + rho*AL**2\n",
    "    \n",
    "    return f, rho\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abf81033",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "#######  Physics Informed Neural N.  ########\n",
    "#############################################\n",
    "\n",
    "\n",
    "def L_layer_model(X0, Y0, XbcL, XbcR, Xpde, \n",
    "                  layers_dims, learning_rate, num_iterations, bulk_activation):\n",
    "    \"\"\"\n",
    "    This function solves the physics informed neural network\n",
    "    \n",
    "    Arguments:\n",
    "    Y0 --   exact solution for the initial condition \n",
    "    X0 --   input data for the initial condition\n",
    "    XbcL -- input data for the left boundary condition \n",
    "    XbcR -- input data for the right boundary condition\n",
    "    Xpde -- input data for the pde condition\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- updated weights and biases\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "                           \n",
    "    # initialization of weights and biases\n",
    "    parameters = initialize_parameters(layers_dims)\n",
    "    \n",
    "    costs = [] \n",
    "\n",
    "    \n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        ##### forward propagation #####\n",
    "        \n",
    "        # initial condition\n",
    "        AL, caches = L_model_forward(X0, parameters, bulk_activation)\n",
    "        \n",
    "        # boundary condition\n",
    "        ALL, cachesL = L_model_forward(XbcL, parameters, bulk_activation)\n",
    "        ALR, cachesR = L_model_forward(XbcR, parameters, bulk_activation)\n",
    "        \n",
    "        # pde condition\n",
    "        ALPDE, cachesPDE = L_model_forward(Xpde, parameters, bulk_activation)\n",
    "        \n",
    "        \n",
    "        ##### pde resolution #####\n",
    "        \n",
    "        f, rho = L_model_pde(ALPDE, cachesPDE, bulk_activation)\n",
    "     \n",
    "        \n",
    "        ##### cost function #####\n",
    "        \n",
    "        \n",
    "        \n",
    "        # cost: initial condition\n",
    "        cost0 = compute_cost(AL, Y0)\n",
    "        \n",
    "        # cost: boundary condition\n",
    "        costBC = compute_cost(ALL, ALR)\n",
    "        \n",
    "        # cost: pde\n",
    "        costPDE = compute_cost(f, 0)\n",
    "        \n",
    "        # cost: all\n",
    "        costALL = cost0 + costBC + costPDE\n",
    "        \n",
    "        # print the cost every 100 training example\n",
    "        if i % 100 == 0:\n",
    "            print (\"cost0, costBC, costPDE, and costALL after iteration %i: %f %f %f %f\" %(i, cost0, costBC, costPDE, costALL))\n",
    "        if i % 100 == 0:\n",
    "            costs.append(costALL)\n",
    "        \n",
    "        \n",
    "        ##### back propagation #####\n",
    "        \n",
    "        # grads: initial condition\n",
    "        grads0 = L_model_backward(AL, Y0, caches, bulk_activation, 'mean')\n",
    "        \n",
    "        # grads: boundary condition -> gradsBC = gradsL + gradsR\n",
    "        gradsL = L_model_backward(ALL, ALR, cachesL, bulk_activation, 'mean')\n",
    "        gradsR = L_model_backward(ALR, ALL, cachesR, bulk_activation, 'mean')\n",
    "        \n",
    "        gradsBC = {}\n",
    "        \n",
    "        for l in range(1, len(layers_dims)):\n",
    "            gradsBC['dW' + str(l)] = gradsL['dW' + str(l)] + gradsR['dW' + str(l)]\n",
    "            gradsBC['db' + str(l)] = gradsL['db' + str(l)] + gradsR['db' + str(l)]\n",
    "        \n",
    "            assert(gradsBC['dW' + str(l)].shape == (layers_dims[l], layers_dims[l-1]))\n",
    "            assert(gradsBC['db' + str(l)].shape == (layers_dims[l], 1))\n",
    "            \n",
    "        \n",
    "        # grads: pde -> grads(du/dt) - rho*grads(u) + rho*grads(u**2) = gradsPDE1 + gradsPDE2 + gradsPDE3\n",
    "        gradsPDE1 = L_model_backward_pde(ALPDE, f, cachesPDE, bulk_activation, 'mean', 'pde1', rho)\n",
    "        gradsPDE2 = L_model_backward_pde(ALPDE, f, cachesPDE, bulk_activation, 'mean', 'pde2', rho)\n",
    "        gradsPDE3 = L_model_backward_pde(ALPDE, f, cachesPDE, bulk_activation, 'mean', 'pde3', rho) \n",
    "\n",
    "        gradsPDE = {}\n",
    "        for l in range(1, len(layers_dims)):\n",
    "            gradsPDE['dW' + str(l)] = gradsPDE1['dW' + str(l)] + gradsPDE2['dW' + str(l)] + gradsPDE3['dW' + str(l)]\n",
    "            gradsPDE['db' + str(l)] = gradsPDE1['db' + str(l)] + gradsPDE2['db' + str(l)] + gradsPDE3['db' + str(l)]\n",
    "        \n",
    "            assert(gradsPDE['dW' + str(l)].shape == (layers_dims[l], layers_dims[l-1]))\n",
    "            assert(gradsPDE['db' + str(l)].shape == (layers_dims[l], 1))\n",
    "        \n",
    "        \n",
    "        # grads: all\n",
    "        grads = {}\n",
    "        for l in range(1, len(layers_dims)):\n",
    "            grads['dW' + str(l)] = grads0['dW' + str(l)] + gradsBC['dW' + str(l)] + gradsPDE['dW' + str(l)]\n",
    "            grads['db' + str(l)] = grads0['db' + str(l)] + gradsBC['db' + str(l)] + gradsPDE['db' + str(l)]\n",
    "        \n",
    "            assert(grads['dW' + str(l)].shape == (layers_dims[l], layers_dims[l-1]))\n",
    "            assert(grads['db' + str(l)].shape == (layers_dims[l], 1))\n",
    "        \n",
    "        \n",
    "        ##### gradient descent #####\n",
    "        \n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "      \n",
    "                     \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost_all')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebb0cee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost0, costBC, costPDE, and costALL after iteration 0: 0.201095 0.002370 0.008990 0.212454\n",
      "cost0, costBC, costPDE, and costALL after iteration 100: 0.105549 0.001785 0.006862 0.114196\n",
      "cost0, costBC, costPDE, and costALL after iteration 200: 0.099334 0.001053 0.005647 0.106034\n",
      "cost0, costBC, costPDE, and costALL after iteration 300: 0.093044 0.000794 0.005089 0.098927\n",
      "cost0, costBC, costPDE, and costALL after iteration 400: 0.087049 0.000603 0.004736 0.092387\n",
      "cost0, costBC, costPDE, and costALL after iteration 500: 0.081329 0.000441 0.004505 0.086275\n",
      "cost0, costBC, costPDE, and costALL after iteration 600: 0.075862 0.000308 0.004365 0.080535\n",
      "cost0, costBC, costPDE, and costALL after iteration 700: 0.070663 0.000204 0.004298 0.075165\n",
      "cost0, costBC, costPDE, and costALL after iteration 800: 0.065783 0.000129 0.004291 0.070203\n",
      "cost0, costBC, costPDE, and costALL after iteration 900: 0.061294 0.000081 0.004335 0.065710\n",
      "cost0, costBC, costPDE, and costALL after iteration 1000: 0.057265 0.000060 0.004417 0.061742\n",
      "cost0, costBC, costPDE, and costALL after iteration 1100: 0.053738 0.000061 0.004520 0.058320\n",
      "cost0, costBC, costPDE, and costALL after iteration 1200: 0.050713 0.000083 0.004622 0.055418\n",
      "cost0, costBC, costPDE, and costALL after iteration 1300: 0.048149 0.000122 0.004702 0.052972\n",
      "cost0, costBC, costPDE, and costALL after iteration 1400: 0.045983 0.000173 0.004748 0.050904\n",
      "cost0, costBC, costPDE, and costALL after iteration 1500: 0.044148 0.000234 0.004756 0.049138\n",
      "cost0, costBC, costPDE, and costALL after iteration 1600: 0.042581 0.000298 0.004731 0.047610\n",
      "cost0, costBC, costPDE, and costALL after iteration 1700: 0.041228 0.000363 0.004681 0.046271\n",
      "cost0, costBC, costPDE, and costALL after iteration 1800: 0.040045 0.000425 0.004614 0.045084\n",
      "cost0, costBC, costPDE, and costALL after iteration 1900: 0.038999 0.000483 0.004537 0.044019\n",
      "cost0, costBC, costPDE, and costALL after iteration 2000: 0.038064 0.000534 0.004458 0.043056\n",
      "cost0, costBC, costPDE, and costALL after iteration 2100: 0.037219 0.000579 0.004380 0.042178\n",
      "cost0, costBC, costPDE, and costALL after iteration 2200: 0.036448 0.000617 0.004306 0.041371\n",
      "cost0, costBC, costPDE, and costALL after iteration 2300: 0.035740 0.000648 0.004237 0.040624\n",
      "cost0, costBC, costPDE, and costALL after iteration 2400: 0.035084 0.000672 0.004173 0.039929\n",
      "cost0, costBC, costPDE, and costALL after iteration 2500: 0.034474 0.000690 0.004115 0.039278\n",
      "cost0, costBC, costPDE, and costALL after iteration 2600: 0.033903 0.000702 0.004061 0.038665\n",
      "cost0, costBC, costPDE, and costALL after iteration 2700: 0.033366 0.000708 0.004010 0.038084\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 4-layer model with 2 input parameters\n",
    "layers_dims = [2, 50, 50, 50, 1] \n",
    "\n",
    "# Training model\n",
    "parameters = L_layer_model(train_X0, train_Y0, train_Xbc_L, train_Xbc_R, train_Xpde, \n",
    "                           layers_dims, learning_rate = 0.0044, num_iterations = 2800, bulk_activation='tanh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb8060dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[-0.39974627, -0.03571868],\n",
       "        [-1.51434805,  1.15950721],\n",
       "        [-1.26840384, -0.59483974],\n",
       "        [ 0.42577419, -0.85959776],\n",
       "        [-0.7259933 , -0.6346835 ],\n",
       "        [ 0.3658912 ,  1.62579371],\n",
       "        [-0.0801714 , -0.78052405],\n",
       "        [ 0.34989089, -0.42987987],\n",
       "        [-0.03346482,  0.83280223],\n",
       "        [-0.50554026,  0.01483126],\n",
       "        [-0.58276288, -0.10042527],\n",
       "        [ 0.21729977, -0.66586169],\n",
       "        [-0.13339415, -0.15898113],\n",
       "        [-0.4482073 , -0.81746711],\n",
       "        [-1.02825312, -0.11383897],\n",
       "        [-0.11901496,  1.56976646],\n",
       "        [-1.71812246,  0.07657057],\n",
       "        [ 0.21439563,  0.96736204],\n",
       "        [ 0.25735692, -0.58392082],\n",
       "        [-0.14258987,  0.35800771],\n",
       "        [-0.18588913,  0.49759721],\n",
       "        [-1.3227    ,  1.23131605],\n",
       "        [ 1.03000789, -0.2393958 ],\n",
       "        [ 0.41745038,  0.02734009],\n",
       "        [-0.57154583,  0.06240265],\n",
       "        [ 0.68658126, -0.27590153],\n",
       "        [-0.25154105, -0.05298003],\n",
       "        [ 0.24872206,  0.90640589],\n",
       "        [-0.48574724,  0.3510519 ],\n",
       "        [ 0.15670462, -1.31710651],\n",
       "        [-0.23618155, -0.09261612],\n",
       "        [-0.00900159,  0.22720158],\n",
       "        [-1.44249047,  0.03186065],\n",
       "        [-0.48295249, -1.01216338],\n",
       "        [ 0.36089829,  0.52681949],\n",
       "        [-0.43085289,  0.59887575],\n",
       "        [-0.44064844,  0.03832034],\n",
       "        [-0.77038665,  1.12393726],\n",
       "        [-1.87813776, -0.06690504],\n",
       "        [ 0.49668708, -1.42800795],\n",
       "        [-0.13679719, -0.03891486],\n",
       "        [ 0.55666334,  0.87077763],\n",
       "        [-0.33620278, -0.963563  ],\n",
       "        [ 0.9860278 ,  0.8651799 ],\n",
       "        [-0.33726652,  0.24273828],\n",
       "        [ 0.35083031,  0.39705735],\n",
       "        [ 0.11004412,  0.99334383],\n",
       "        [-1.24451243,  0.72639809],\n",
       "        [ 0.29285501, -0.14823122],\n",
       "        [ 0.82899892, -1.65823876]]),\n",
       " 'b1': array([[ 0.02805786],\n",
       "        [ 0.00047928],\n",
       "        [-0.00351228],\n",
       "        [-0.00150829],\n",
       "        [-0.00080773],\n",
       "        [ 0.0255939 ],\n",
       "        [ 0.02788437],\n",
       "        [-0.01603668],\n",
       "        [-0.0109816 ],\n",
       "        [ 0.02876707],\n",
       "        [ 0.02127055],\n",
       "        [-0.02046234],\n",
       "        [ 0.01975512],\n",
       "        [ 0.03168384],\n",
       "        [-0.00148884],\n",
       "        [ 0.00672699],\n",
       "        [-0.00648311],\n",
       "        [-0.01943509],\n",
       "        [ 0.014243  ],\n",
       "        [-0.01043619],\n",
       "        [ 0.04111895],\n",
       "        [ 0.00874319],\n",
       "        [-0.00786819],\n",
       "        [-0.00688485],\n",
       "        [ 0.03808304],\n",
       "        [-0.00667333],\n",
       "        [ 0.00922301],\n",
       "        [-0.01746281],\n",
       "        [ 0.02822153],\n",
       "        [-0.00995829],\n",
       "        [ 0.00980767],\n",
       "        [ 0.00125482],\n",
       "        [-0.0045661 ],\n",
       "        [ 0.0071822 ],\n",
       "        [ 0.01772651],\n",
       "        [ 0.02141174],\n",
       "        [ 0.01664505],\n",
       "        [ 0.00326787],\n",
       "        [-0.0081584 ],\n",
       "        [-0.01076502],\n",
       "        [ 0.03376258],\n",
       "        [-0.03085026],\n",
       "        [-0.0085258 ],\n",
       "        [-0.03561291],\n",
       "        [ 0.02713289],\n",
       "        [ 0.00195863],\n",
       "        [ 0.03044388],\n",
       "        [-0.01448037],\n",
       "        [ 0.0050714 ],\n",
       "        [ 0.00436383]]),\n",
       " 'W2': array([[ 0.08289563, -0.00582268, -0.04364933, ...,  0.0705968 ,\n",
       "          0.1475679 , -0.04291083],\n",
       "        [-0.1812163 , -0.0640377 , -0.01438399, ...,  0.05049639,\n",
       "          0.08294531, -0.05561166],\n",
       "        [-0.03224519,  0.07360307, -0.00904075, ..., -0.07223904,\n",
       "         -0.01852231, -0.03122664],\n",
       "        ...,\n",
       "        [ 0.18608643, -0.00761717, -0.21091688, ...,  0.12721571,\n",
       "          0.19327195, -0.15695574],\n",
       "        [-0.01176092,  0.08388445,  0.0975129 , ...,  0.03510227,\n",
       "         -0.05106603,  0.09704912],\n",
       "        [-0.04873399,  0.0819305 , -0.0774851 , ...,  0.24233174,\n",
       "          0.01390142,  0.03423897]]),\n",
       " 'b2': array([[ 0.02104963],\n",
       "        [ 0.00835507],\n",
       "        [ 0.01466791],\n",
       "        [ 0.01502329],\n",
       "        [ 0.01439132],\n",
       "        [ 0.00021665],\n",
       "        [ 0.00395237],\n",
       "        [-0.00828558],\n",
       "        [-0.00201099],\n",
       "        [ 0.01332341],\n",
       "        [ 0.01153979],\n",
       "        [-0.02137054],\n",
       "        [ 0.00363728],\n",
       "        [ 0.01634381],\n",
       "        [-0.00749061],\n",
       "        [ 0.00890638],\n",
       "        [-0.00373531],\n",
       "        [ 0.0034282 ],\n",
       "        [-0.01171771],\n",
       "        [ 0.01262696],\n",
       "        [-0.01578969],\n",
       "        [ 0.0046278 ],\n",
       "        [ 0.00725433],\n",
       "        [ 0.00517394],\n",
       "        [-0.01997716],\n",
       "        [-0.0097238 ],\n",
       "        [ 0.02511937],\n",
       "        [-0.00473585],\n",
       "        [ 0.00896423],\n",
       "        [-0.00217345],\n",
       "        [ 0.01815076],\n",
       "        [ 0.00153083],\n",
       "        [ 0.01433324],\n",
       "        [-0.02147414],\n",
       "        [-0.00228254],\n",
       "        [-0.01073211],\n",
       "        [ 0.00155331],\n",
       "        [ 0.01612072],\n",
       "        [ 0.00257238],\n",
       "        [ 0.00979152],\n",
       "        [ 0.0295843 ],\n",
       "        [ 0.0005993 ],\n",
       "        [ 0.00796353],\n",
       "        [ 0.00864571],\n",
       "        [ 0.01061255],\n",
       "        [ 0.00815171],\n",
       "        [ 0.00335592],\n",
       "        [ 0.00834315],\n",
       "        [-0.01028394],\n",
       "        [ 0.01116744]]),\n",
       " 'W3': array([[-0.00827489, -0.1262242 , -0.00445578, ...,  0.03576809,\n",
       "          0.03971191, -0.02790495],\n",
       "        [-0.03083571, -0.31700709,  0.04447346, ..., -0.39784816,\n",
       "          0.23300144, -0.05259538],\n",
       "        [-0.07289093, -0.08262269, -0.23838434, ..., -0.16846116,\n",
       "          0.04670711, -0.01442141],\n",
       "        ...,\n",
       "        [ 0.07376912,  0.0386813 , -0.0904692 , ..., -0.17555917,\n",
       "         -0.04506724,  0.11876931],\n",
       "        [-0.05459302, -0.05602902,  0.15331738, ..., -0.15659147,\n",
       "         -0.10139343,  0.33226672],\n",
       "        [ 0.11473133,  0.03389119,  0.13710593, ..., -0.04102844,\n",
       "         -0.01605295, -0.02368227]]),\n",
       " 'b3': array([[-0.00080376],\n",
       "        [ 0.0043946 ],\n",
       "        [ 0.00890773],\n",
       "        [-0.00196659],\n",
       "        [ 0.00180962],\n",
       "        [-0.00069446],\n",
       "        [-0.01091119],\n",
       "        [-0.01148105],\n",
       "        [ 0.00124203],\n",
       "        [-0.00328935],\n",
       "        [-0.00726489],\n",
       "        [ 0.00939969],\n",
       "        [ 0.00862082],\n",
       "        [-0.0358089 ],\n",
       "        [-0.00562156],\n",
       "        [ 0.00991282],\n",
       "        [ 0.01482706],\n",
       "        [-0.00112038],\n",
       "        [-0.00967272],\n",
       "        [-0.00838468],\n",
       "        [-0.00682939],\n",
       "        [ 0.01079223],\n",
       "        [-0.0130992 ],\n",
       "        [ 0.00065011],\n",
       "        [ 0.00075951],\n",
       "        [ 0.00809839],\n",
       "        [ 0.00151905],\n",
       "        [-0.00795157],\n",
       "        [ 0.0044935 ],\n",
       "        [-0.00508361],\n",
       "        [ 0.00225364],\n",
       "        [-0.00450365],\n",
       "        [-0.01563093],\n",
       "        [-0.00251767],\n",
       "        [-0.01202277],\n",
       "        [ 0.00939695],\n",
       "        [ 0.01161203],\n",
       "        [ 0.0170763 ],\n",
       "        [-0.02065763],\n",
       "        [ 0.00035126],\n",
       "        [-0.01873068],\n",
       "        [-0.00491655],\n",
       "        [-0.01094692],\n",
       "        [-0.03994953],\n",
       "        [-0.00107036],\n",
       "        [-0.02353462],\n",
       "        [ 0.01372974],\n",
       "        [-0.0018542 ],\n",
       "        [-0.00945802],\n",
       "        [-0.00834149]]),\n",
       " 'W4': array([[-0.08922466,  0.15141995,  0.231458  ,  0.08001176,  0.05723345,\n",
       "          0.3074788 , -0.01781805,  0.00976446, -0.23716117, -0.24713588,\n",
       "         -0.18669246,  0.13465966,  0.40386429, -0.24375446,  0.0189799 ,\n",
       "          0.15134307,  0.05086274,  0.31071539, -0.2494788 , -0.05502063,\n",
       "         -0.14899541,  0.29340439,  0.20158937,  0.0904754 ,  0.10684349,\n",
       "          0.15017886,  0.27432315,  0.11844446, -0.27150071, -0.04439178,\n",
       "          0.16894949,  0.03095564, -0.15310636,  0.03440309,  0.04655284,\n",
       "         -0.29651037, -0.00543196, -0.01757343, -0.1306809 ,  0.11354622,\n",
       "          0.07682403, -0.12284087, -0.10608004, -0.22989152,  0.14601707,\n",
       "         -0.16804002,  0.11432172,  0.19762227, -0.08917692,  0.15635221]]),\n",
       " 'b4': array([[0.03061063]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7956378",
   "metadata": {},
   "outputs": [],
   "source": [
    "AL, caches = L_model_forward(predict_X, parameters, bulk_activation='tanh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74cf391d",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_abs = np.mean(np.abs(predict_Y - AL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d970875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15960743117699303"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b9e0290",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "\"\"\"Visualize exact solution.\"\"\"\n",
    "fig = plt.figure(figsize=(9, 5))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "h = ax.imshow(ugrid.T, interpolation='nearest', cmap='rainbow',\n",
    "              extent=[t.min(), t.max(), x.min(), x.max()],\n",
    "              origin='lower', aspect='auto')\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.10)\n",
    "cbar = fig.colorbar(h, cax=cax)\n",
    "cbar.ax.tick_params(labelsize=15)\n",
    "\n",
    "line = np.linspace(x.min(), x.max(), 2)[:,None]\n",
    "\n",
    "ax.set_xlabel('t', fontweight='bold', size=30)\n",
    "ax.set_ylabel('x', fontweight='bold', size=30)\n",
    "ax.legend(\n",
    "    loc='upper center',\n",
    "    bbox_to_anchor=(0.9, -0.05),\n",
    "    ncol=5,\n",
    "    frameon=False,\n",
    "    prop={'size': 15}\n",
    ")\n",
    "\n",
    "ax.tick_params(labelsize=15)\n",
    "#def exact_u(ugrid, x, t):\n",
    "    \n",
    "\n",
    "    #plt.savefig(f\"{path}/exactu_{system}_nu{nu}_beta{beta}_rho{rho}_Nf{N_f}_{layers}_L{L}_source{source}_{u0_str}.pdf\")\n",
    "    #plt.close()\n",
    "\n",
    "#    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d40e43a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "upredict = AL.T\n",
    "\"\"\"Visualize exact solution.\"\"\"\n",
    "fig = plt.figure(figsize=(9, 5))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "h = ax.imshow(upredict.T, interpolation='nearest', cmap='rainbow',\n",
    "              extent=[t.min(), t.max(), x.min(), x.max()],\n",
    "              origin='lower', aspect='auto')\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.10)\n",
    "cbar = fig.colorbar(h, cax=cax)\n",
    "cbar.ax.tick_params(labelsize=15)\n",
    "\n",
    "line = np.linspace(x.min(), x.max(), 2)[:,None]\n",
    "\n",
    "ax.set_xlabel('t', fontweight='bold', size=30)\n",
    "ax.set_ylabel('x', fontweight='bold', size=30)\n",
    "ax.legend(\n",
    "    loc='upper center',\n",
    "    bbox_to_anchor=(0.9, -0.05),\n",
    "    ncol=5,\n",
    "    frameon=False,\n",
    "    prop={'size': 15}\n",
    ")\n",
    "\n",
    "ax.tick_params(labelsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcda546c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
